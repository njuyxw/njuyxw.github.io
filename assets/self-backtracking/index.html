
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Add MathJax script -->
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <title>Self-Backtracking</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet"> -->

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script> -->
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
<body>

  <!-- Title -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Step Back to Leap Forward: <br>Self-Backtracking for Boosting Reasoning of Language Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/yangxw/">Xiao-Wen Yang<sup>1 2</sup></a>,</span>
              <span class="author-block"><a href="#">Xuan-Yi Zhu<sup>1 2</sup></a>,</span>
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/weiwd/">Wen-Da Wei<sup>1 2</sup></a>,</span>
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/zhangdc/">Ding-Chu Zhang<sup>1 2</sup></a>,</span>
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/shaojj/">Jie-Jing Shao<sup>1</sup></a>,</span><br>
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/zhouz/">Zhi Zhou<sup>1</sup></a>,</span>
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/guolz/">Lan-Zhe Guo<sup>1 3</sup></a>,</span>
              <span class="author-block"><a href="https://www.lamda.nju.edu.cn/liyf/">Yu-Feng Li<sup>1 2</sup></a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>National Key Laboratory for Novel Software Technology, Nanjing University, China<br></span>
              <span class="author-block"><sup>2</sup>School of Artificial Intelligence, Nanjing University, China<br></span>
              <span class="author-block"><sup>3</sup>School of Intelligence Science and Technology, Nanjing University, China<br></span>
            </div>
            
            <span class="author-block"><a href="mailto:yangxw@lamda.nju.edu.cn">yangxw@lamda.nju.edu.cn</a></span>

            <!-- Links -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.04404" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                <span class="link-block">
                  <a href="https://github.com/LAMDASZ-ML/Self-Backtracking" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                    </span>
                    <span>GitHub</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/yangxw/countdown-backtracking" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">ðŸ¤—</span>
                    <span>Data</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/yangxw/Llama-3.2-1B-countdown-backtrack" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">ðŸ¤—</span>
                    <span>Model</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- After the Abstract section and before the Method section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <figure class="image">
            <img src="static/images/method.jpg" alt="Overview of Self-Backtracking Method">
            <figcaption class="has-text-centered">
            Figure 1: The overall framework of Self-Backtracking. 
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>

  <!-- Method -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Method</h2>
      <div class="content">
        <p>Under the standard Supervised Fine-Tuning (SFT) framework, we typically employ a dataset \(\mathcal{D}_{op} = \{(x_i, y_i)\}_{i \in [n_{op}]}\), where for reasoning tasks, \(y_i\) represents the natural language reasoning path representing the optimal solution. To enable the model to backtrack at appropriate times and positions, we introduce a backtracking dataset:  \[\mathcal{D}_{back} = \{\left(x_j, \texttt{prefix}(y_j) \circ a_{err} \circ \langle \texttt{backtrack} \rangle\right)\}_{j \in [n]}.\] 
          Here, \(\texttt{prefix}(y_j)\) denotes the prefix of the optimal solution \(y_j\), representing a partial solution; \(a_{err}\) signifies an erroneous action extended from the partial solution, which cannot lead to the correct answer; and \(\langle \texttt{backtrack} \rangle\) is a special token indicating that the model needs to backtrack from the current state. The final dataset is \(\mathcal{D}=\mathcal{D}_{op}\cup\mathcal{D}_{back}\). </p>
        
        <p>Our self-backtracking technique consists of three main phases:</p>
        
        <h3 class="title is-4">1. Training Phase</h3>
        <p>We train the model using a combination of optimal solution data and backtracking data:</p>
        <div class="">
          <!-- Use MathJax delimiters for inline math -->
          \[ \mathcal{L}(\theta) = \mathcal{L}_{SFT}(\theta)+\mathcal{L}_{backtrack}(\theta) \]
        </div>
        <p>The training loss function consists of two components: the supervised fine-tuning (SFT) loss and the backtracking loss. The backtracking dataset is constructed to help the model learn when and where to backtrack by introducing erroneous actions and a special backtrack token.</p>
        <div class="">
          <!-- Use MathJax delimiters for block math -->
          \[
          \mathcal{L}_{backtrack}(\theta) = -\frac{1}{n_{back}}\sum_{j=1}^{n_{back}}\log p_{\theta}(\texttt{prefix}(y_j)|x_j)
          -\frac{1}{n_{back}}\sum_{j=1}^{n_{back}} \log p_{\theta}(\langle \texttt{backtrack} \rangle | x_j \circ \texttt{prefix}(y_j) \circ a_{err})
          \]
        </div>
        <p>The backtracking loss function, \(\mathcal{L}_{backtrack}(\theta)\), is crucial for training the model to recognize when its current reasoning path is suboptimal and to backtrack to explore alternative paths. It consists of two main components:</p>
        <ul>
          <li><strong>Partial Solution Prediction:</strong> This component encourages the model to predict partial solutions accurately given the input, ensuring that the model can generate correct reasoning steps up to a certain point.</li>
          <li><strong>Backtrack Token Prediction:</strong> This component focuses on the model's ability to predict the \(\langle \texttt{backtrack} \rangle\) token when it has deviated from the correct path. This helps the model learn to identify erroneous actions and backtrack appropriately.</li>
        </ul>
        <p>Here, \(\texttt{prefix}(y_j)\) represents a partial solution, and \(a_{err}\) is an erroneous action. The \(\langle \texttt{backtrack} \rangle\) token indicates that the model should backtrack from the current state.</p>
        
        <h3 class="title is-4">2. Inference Phase</h3>
        <p>During inference, we employ a novel search algorithm that considers both depth and breadth, consisting of three steps:</p>
        <ul>
          <li><strong>Expansion:</strong> Sample N predictions and categorize them</li>
          <li><strong>Backtracking:</strong> Process backtracking containing backtrack tokens</li>
          <li><strong>Selection:</strong> Choose the best reasoning path based on perplexity scores</li>
        </ul>
        <p>This algorithm leverages the learned backtracking capabilities without requiring external reward models, maintaining controllable computational costs.</p>

        <h3 class="title is-4">3. Self-Improvement Phase</h3>
        <p> In this stage we aim to transfer the model's slow thinking abilities to fast thinking through the self-improvement method. To achieve this, we employ an expert iteration strategy, which primarily consists of three steps: First, during the slow thinking data generation phase, we utilize the self-backtracking inference model to produce high-quality reasoning path data. Subsequently, in the expert screening phase, experts evaluate the generated data to select training samples suitable for the fast thinking model. In our experiment, we quantify the model's accuracy using an evaluator. Finally, in the fast thinking model training phase, the selected high-quality data is used to train the fast thinking model by SFT. Through this iterative optimization, we get continuous enhancement in the performance of the fast thinking model. </p>
      </div>
    </div>
  </section>

  <!-- Results -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Results</h2>
      <div class="content">
      <p>We employ the Countdown task as our principal experimental framework to rigorously evaluate the reasoning capabilities of our self-backtracking approach. This task extends the traditional 24 Game by necessitating that LLMs strategically combine a provided set of input numbers using fundamental arithmetic operationsâ€”addition, subtraction, multiplication, and divisionâ€”to achieve a predefined target number. The complexity of the task stems from its expansive search space, which rigorously tests the models' ability in reasoning the correct path.</p>
    </div>
      <div class="content">
        <h3 class="title is-4">Main Results</h3>
      <div class="content">
        <p>We present the accuracy of various methods across different models for the countdown task. Overall, the self-backtracking technique demonstrates a significant improvement over the baseline of greedy search after SFT, with enhancements of approximately 40% on Llama3.2-1B and over 30% on Llama3.2-3B.</p>
      <div class="columns is-centered">
        <div class="column">
          <figure class="image">
            <img src="static/images/main_res.png" alt="Self-improvement results">
            <!-- <figcaption class="has-text-centered"></figcaption> -->
          </figure>
        </div>
      </div>
    </div>

      <div class="content">
        <h3 class="title is-4">Analysis</h3>
        
      <div class="content">
        <p>We conduct experiments by varying the \(b\) and \(N\), and generate performance curves under different \(b\) values as \(N\) increases, as illustrated in Figure 2. The results demonstrate that the performance of BoN initially increases and then decreases with larger \(N\), which we attribute to the reward hacking. On the contrary, our method exhibits a consistent improvement with increasing \(N\), eventually stabilizing, indicating a clear test-time scaling law in breadth. Furthermore, when backtracking is permitted (\(b=1\)), the performance improves more rapidly with \(N\) and achieves a higher overall performance, underscoring the necessity of backtracking.</p>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <figure class="image">
            <img class="performance-curve" src="static/images/kncurve.png" alt="Performance curves">
            <figcaption class="has-text-centered">Figure 2: The performance curves when varying N</figcaption>
          </figure>
        </div>
      </div>
      <div class="content">
      <p>Further experiments demonstrate that our algorithm achieves self-improvement through expert iteration. Employing self-backtracking with configurations <code>b=0, N=16</code> and <code>b=1, N=16</code> on two base models and datasets respectively, we filtered correct reasoning paths from inference outputs for SFT. Figure 3 presents three-round improvement results, where bars indicate test performance using our algorithm (slow thinking) and lines represent greedy search (fast thinking). Each iteration brings moderate gains (1-2%)
    </div>
      <div class="columns is-centered">
        <div class="column">
          <figure class="image">
            <img class="performance-curve" src="static/images/improvement_comparison.png" alt="Performance curves">
            <figcaption class="has-text-centered">Figure 3: Self-improvement in accuracy of Llama3.2-1B.</figcaption>
          </figure>
        </div>
      </div>

      <div class="content">
        <p>We conduct experiments to analyze the error types for different \(b\) and \(N\). There are four error types: not reached target, invalid step format, incorrect result in step, and unknown numbers in step. The experimental results shown in Figure 4 demonstrate that our method progressively reduces the proportion of "Not reached target" errors by expanding the search scope. As the parameter \(N\) increases, the model explores more nodes. Allowing more backtracking provides the model with additional opportunities for error correction and retreat, thereby enhancing the probability of finding the correct answer.</p>
      </div>
        <div class="columns is-centered">
          <div class="column">
            <figure class="image">
              <img class="performance-curve" src="static/images/pie_charts_small.png" alt="Performance curves">
              <figcaption class="has-text-centered">Figure 4: Error types of our method for different b and N on Seen Targets of Llama3.2-1B.</figcaption>
            </figure>
          </div>
      </div>
    </div>
    </div>
  </section>

  <!-- Citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@article{selfbacktracking,
  title={Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models},
  author={Xiao-Wen Yang and Xuan-Yi Zhu and Wen-Da Wei and Ding-Chu Zhang and Jie-Jing Shao and Zhi Zhou and Lan-Zhe Guo and Yu-Feng Li},
  journal={arXiv preprint arXiv:2502.04404},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </footer>

</body>
</html>
